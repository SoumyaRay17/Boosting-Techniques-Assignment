{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **01.What is Boosting in Machine Learning? Explain how it improves weak learners.**\n",
        "\n",
        "Boosting is an ensemble learning technique that sequentially combines multiple weak classifiers to create a strong classifier. It is done by training a model using training data and is then evaluated. Next model is built on that which tries to correct the errors present in the first model. This procedure is continued and models are added until either the complete training data set is predicted correctly or predefined number of iterations is reached.\n",
        "\n",
        "Advantages of Boosting :-\n",
        "\n",
        "Improved Accuracy:\n",
        "\n",
        "By combining multiple weak learners it enhances predictive accuracy for both classification and regression tasks.\n",
        "\n",
        "Robustness to Overfitting:\n",
        "\n",
        "Unlike traditional models it dynamically adjusts weights to prevent overfitting.\n",
        "Handles Imbalanced Data Well: It prioritizes misclassified points making it effective for imbalanced datasets.\n",
        "\n",
        "Better Interpretability:\n",
        "\n",
        "The sequential nature of helps break down decision-making making the model more interpretable.\n",
        "\n",
        "Here's how boosting improves weak learners:\n",
        "\n",
        "Boosting converts a system of weak learners into a single strong learning system. For example, to identify the cat image, it combines a weak learner that guesses for pointy ears and another learner that guesses for cat-shaped eyes.\n",
        "\n",
        "Sequential Training:\n",
        "\n",
        "Unlike bagging, where models are trained independently, boosting trains models one after the other.\n",
        "\n",
        "Error Correction:\n",
        "\n",
        "Each new model in the boosting sequence gives more weight to the instances that were misclassified by the previous model(s). This allows the boosting algorithm to focus on the most challenging parts of the data and learn complex patterns.\n",
        "\n",
        "Weight Adjustment:\n",
        "\n",
        "The algorithm assigns higher weights to misclassified data points, ensuring that subsequent models pay more attention to these instances during training.\n",
        "\n",
        "Bias Reduction:\n",
        "\n",
        "By iteratively focusing on errors, boosting helps reduce bias, leading to a model that is more likely to generalize well to unseen data.\n",
        "\n",
        "Variance Reduction:\n",
        "\n",
        "While primarily focused on bias reduction, boosting can also lead to a reduction in variance, especially in complex models.\n",
        "\n",
        "In essence, boosting leverages the combined power of multiple weak learners, each contributing a slightly better-than-random prediction. Through the iterative process of learning from past errors and focusing on difficult instances, boosting transforms these weak learners into a strong, accurate, and robust predictive model."
      ],
      "metadata": {
        "id": "tZyIqXqLeYAw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **02.What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?**\n",
        "\n",
        "AdaBoost :-\n",
        "\n",
        "AdaBoost is a boosting algorithm, which also works on the principle of the stagewise addition method where multiple weak learners are used for getting strong learners. Unlike Gradient Boosting in XGBoost, the alpha parameter I calculated is related to the errors of the weak learner, here the value of the alpha parameter will be indirectly proportional to the error of the weak learner.\n",
        "\n",
        "Once the alpha parameter is calculated, the weightage will be given to the particular weak learners, here the weak learner that are doing mistakes will get more weightage to fill out the gap in error and the weak learners that are already performing well will get fewer weights as they are already a good model.\n",
        "\n",
        "AdaBoost (Adaptive Boosting):\n",
        "\n",
        "Focus on Misclassified Data:\n",
        "\n",
        "AdaBoost assigns higher weights to data points that were misclassified by previous models in the sequence.\n",
        "\n",
        "Sequential Model Training:\n",
        "\n",
        "Each new model in the sequence is trained on the weighted data, aiming to correct the errors of the previous models.\n",
        "\n",
        "Weighted Voting:\n",
        "\n",
        "Predictions from all models in the sequence are combined using a weighted vote, where models that performed better on the weighted data have a greater influence.\n",
        "\n",
        "Example:\n",
        "\n",
        "Imagine a decision tree stump (a simple tree with only one split) is created. If some data points are misclassified, AdaBoost increases their weights, and the next tree will try to correct those errors.\n",
        "\n",
        "Gradient Boosting :-\n",
        "\n",
        "Gradient Boosting is the boosting algorithm that works on the principle of the stagewise addition method, where multiple weak learning algorithms are trained and a strong learner algorithm is used as a final model from the addition of multiple weak learning algorithms trained on the same dataset.\n",
        "\n",
        "In the gradient boosting algorithm, the first weak learner will not be trained on the dataset, it will simply return the mean of the particular column, and the residual for output of the first weak learner algorithm will be calculated which will be used as output or target column for next weak learning algorithm which is to be trained.\n",
        "\n",
        "Following the same pattern, the second weak learner will be trained and the residuals will be calculated which will be used as an output column again for the next weak learner, this is how this process will continue until we reach zero residuals.\n",
        "\n",
        "In gradient boosting the dataset should be in the form of numerical or categorical data and the loss function using which the residuals are calculated should be differential at all points.\n",
        "\n",
        "\n",
        "Gradient Boosting:\n",
        "\n",
        "Focus on Loss Function:\n",
        "\n",
        "Gradient Boosting minimizes a chosen loss function (e.g., for regression, it could be mean squared error).\n",
        "\n",
        "Sequential Model Training:\n",
        "\n",
        "Each new model is trained to predict the negative gradient of the loss function with respect to the previous model's predictions.\n",
        "\n",
        "Additive Modeling:\n",
        "\n",
        "The predictions of each new model are added to the previous model's predictions (with a scaling factor) to gradually improve the overall prediction.\n",
        "\n",
        "Example:\n",
        "\n",
        "If a model initially predicts a value that's too high (resulting in a positive gradient), the next model will be trained to reduce the error by predicting a smaller value.\n",
        "\n",
        "Flexibility:\n",
        "\n",
        "Gradient Boosting is more flexible because it can work with various loss functions, making it suitable for both regression and classification tasks\n"
      ],
      "metadata": {
        "id": "XwtlXmKFfZxW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **03.How does regularization help in XGBoost?**\n",
        "\n",
        "Regularization in XGBoost helps prevent overfitting and improve the model's generalization performance on unseen data. It achieves this by adding penalty terms to the objective function, which discourages overly complex models.\n",
        "\n",
        "Here's a more detailed breakdown: Objective Function with Regularization.\n",
        "\n",
        "XGBoost's objective function includes both a loss term (measuring how well the model fits the training data) and a regularization term (penalizing model complexity). The goal is to minimize this combined objective.\n",
        "\n",
        "Types of Regularization:\n",
        "\n",
        "L1 Regularization (Lasso - alpha parameter): This adds a penalty proportional to the absolute value of the leaf weights. It encourages sparsity, meaning it can drive the weights of less important features to zero, effectively performing feature selection.\n",
        "\n",
        "L2 Regularization (Ridge - lambda parameter): This adds a penalty proportional to the square of the leaf weights. It encourages smaller, more balanced weights across features, preventing any single feature from dominating the model.\n",
        "\n",
        "Gamma (gamma parameter):\n",
        "\n",
        " This parameter controls the minimum loss reduction required to make a further split in a tree. A higher gamma value leads to more conservative tree growth and earlier pruning, preventing the creation of branches that provide only marginal gain in the loss function.\n",
        "\n",
        "How it Helps Prevent Overfitting:\n",
        "\n",
        "Penalizing Complexity: The regularization terms directly penalize models with a large number of leaves or large leaf weights, forcing the algorithm to find simpler structures.\n",
        "\n",
        "Discouraging Fitting Noise:\n",
        "\n",
        "By penalizing complexity, regularization discourages the model from memorizing noise in the training data, which would lead to poor performance on new data.\n",
        "\n",
        "Improving Generalization:\n",
        "\n",
        "A simpler, less overfit model is more likely to capture the underlying patterns in the data and generalize well to unseen examples.\n",
        "\n",
        "Interaction with Tree Pruning:\n",
        "\n",
        "XGBoost builds trees up to a specified max_depth and then prunes them back based on the gain from splits and the gamma parameter. Regularization, particularly gamma, plays a crucial role in this pruning process by determining which splits are deemed beneficial enough to be kept.\n",
        "\n",
        "In essence, regularization in XGBoost provides a mechanism to control the trade-off between model complexity and fitting the training data, ultimately leading to more robust and generalizable models."
      ],
      "metadata": {
        "id": "N29s6o0FhNHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **04.Why is CatBoost considered efficient for handling categorical data?**\n",
        "\n",
        "CatBoost is considered efficient for handling categorical data due to its innovative approaches that address common challenges associated with these features in machine learning, particularly in gradient boosting.\n",
        "\n",
        "CatBoost uses a technique called \"Ordered Target Encoding.\" It assigns values to categories based on target statistics from previous rows – avoiding data leakage! This lets CatBoost handle high-cardinality features (tons of categories) without exploding in size like One-Hot Encoding.\n",
        "\n",
        "Ordered Target Encoding:\n",
        "\n",
        "CatBoost employs a sophisticated form of target encoding called \"Ordered Target Encoding\" or \"Ordered Target Statistics.\" Unlike traditional target encoding which can suffer from target leakage (where information from the target variable is inadvertently used in feature encoding, leading to overfitting), CatBoost addresses this by:\n",
        "\n",
        "Permutation-driven encoding:\n",
        "\n",
        "For each sample, the categorical feature is encoded using target statistics calculated only from the samples that appeared before it in a random permutation of the dataset. This ensures that the encoding for a given sample does not use information from its own target value or values from subsequent samples, preventing target leakage.\n",
        "\n",
        "Bayesian smoothing:\n",
        "\n",
        "This technique further mitigates overfitting by combining the mean target value for a category with the overall target mean, weighted by the number of observations in that category. This helps in cases with low-frequency categories, preventing them from being overly influenced by a few outlier target values.\n",
        "\n",
        "Ordered Boosting:\n",
        "\n",
        "This mechanism is a core innovation that enhances robustness and reduces overfitting. Traditional gradient boosting algorithms can suffer from \"prediction shift\" where the gradients used for training a new tree are calculated using predictions from previous trees that were trained on the same data. CatBoost tackles this by:\n",
        "Random permutations: In each boosting iteration, a random permutation of the dataset is generated.\n",
        "\n",
        "Training on prefixes:\n",
        "\n",
        " When training a new base model, the gradients for a specific sample are calculated using predictions from models trained only on the data points that appear before that sample in the current permutation. This ensures that the gradient calculation is unbiased and prevents the model from \"seeing\" its own target during gradient estimation, leading to more stable and robust models.\n",
        "\n",
        "Handling High Cardinality Categorical Features:\n",
        "\n",
        "CatBoost's encoding schemes are designed to efficiently handle categorical features with a large number of unique values (high cardinality). The ordered target encoding and Bayesian smoothing effectively transform these features into numerical representations without requiring extensive manual preprocessing like one-hot encoding, which can lead to high-dimensional sparse matrices and increased computational cost.\n",
        "\n",
        "Feature Combinations:\n",
        "\n",
        "CatBoost automatically identifies and incorporates combinations of categorical features into the model. This can capture complex interactions between features that might be missed by simple individual feature encodings, leading to improved model performance without requiring manual feature engineering.\n"
      ],
      "metadata": {
        "id": "vfErrxL2hsuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **05.What are some real-world applications where boosting techniques are preferred over bagging methods?**\n",
        "\n",
        "\n",
        "\n",
        "Boosting techniques are often preferred over bagging when high accuracy is paramount and the dataset is complex. Boosting excels at reducing bias and improving performance on challenging datasets, while bagging is more effective at reducing variance and preventing overfitting. Specifically, boosting excels in scenarios where models are inherently weak and require iterative improvement to achieve strong predictive power.\n",
        "\n",
        "Here's a breakdown of why boosting might be preferred in specific situations:\n",
        "\n",
        "1. When High Accuracy is Critical:\n",
        "\n",
        "Boosting algorithms like XGBoost, Gradient Boosting, and AdaBoost are known for their ability to achieve high predictive accuracy, especially on complex datasets.\n",
        "In scenarios where even small improvements in accuracy can have significant real-world impact (e.g., medical diagnosis, fraud detection), boosting is often the preferred choice.\n",
        "\n",
        "For example, in credit scoring, boosting algorithms can analyze a wide range of customer data to predict the likelihood of loan default with greater precision.\n",
        "\n",
        "2. Addressing Bias in Data:\n",
        "\n",
        "Boosting methods are designed to iteratively correct the errors of previous models, which helps reduce bias and improve the model's ability to generalize to new data.\n",
        "Boosting is particularly useful when dealing with datasets that have inherent biases or where some features are more informative than others.\n",
        "\n",
        "3. When Data is Complex and Non-Linear:\n",
        "\n",
        "Boosting algorithms can effectively capture complex, non-linear relationships between features and the target variable, making them suitable for high-dimensional data.\n",
        "For example, in image or object recognition, boosting can be used to identify patterns and features that are difficult to detect with simpler methods.\n",
        "\n",
        "4. Applications:\n",
        "\n",
        "Fraud Detection:\n",
        "\n",
        "Boosting algorithms are often used to identify fraudulent transactions by analyzing patterns and anomalies in financial data.\n",
        "\n",
        "Medical Diagnosis:\n",
        "\n",
        "Boosting can be applied to medical datasets to predict diseases like diabetes or cancer with higher accuracy.\n",
        "\n",
        "Customer Churn Prediction:\n",
        "\n",
        "Boosting can help businesses predict which customers are likely to leave and take proactive measures to retain them.\n",
        "\n",
        "Sentiment Analysis:\n",
        "\n",
        "Boosting can improve the accuracy of sentiment analysis models by considering various factors and their interactions.\n",
        "\n",
        "Financial Forecasting:\n",
        "\n",
        "Boosting can be used to predict stock prices, market trends, and other financial indicators.\n",
        "\n",
        "In contrast, bagging is preferred when:\n",
        "\n",
        "Reducing Variance and Overfitting is the priority:\n",
        "Bagging methods like Random Forest are effective at reducing variance and preventing overfitting, making them suitable for datasets where the model is prone to memorizing the training data.\n",
        "\n",
        "Computational Resources are Limited:\n",
        "\n",
        "Bagging can be less computationally expensive than boosting, especially when dealing with large datasets.\n",
        "In summary, the choice between bagging and boosting depends on the specific requirements of the task. If high accuracy and the ability to model complex relationships are crucial, boosting is often the preferred choice. However, if reducing variance and preventing overfitting are paramount, bagging may be more suitable."
      ],
      "metadata": {
        "id": "s2g-g507icin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#06-\n",
        "#Datasets:\n",
        "#● Use sklearn.datasets.load_breast_cancer() for classification tasks.\n",
        "#● Use sklearn.datasets.fetch_california_housing() for regression tasks.\n",
        "\n",
        "#Question 6: Write a Python program to:\n",
        "#● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "#● Print the model accuracy\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "# Load the breast cancer dataset\n",
        "bc = load_breast_cancer()\n",
        "X = bc.data\n",
        "y = bc.target\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "# Initialize an AdaBoost classifier\n",
        "clf = AdaBoostClassifier()\n",
        "# Train the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "# Make predictions on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "# Print the accuracy score of the classifier\n",
        "print(accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEPRvM144S7t",
        "outputId": "8585ed05-6bae-417c-91af-919395c8cb91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#06-\n",
        "#Datasets:\n",
        "#● Use sklearn.datasets.load_breast_cancer() for classification tasks.\n",
        "#● Use sklearn.datasets.fetch_california_housing() for regression tasks.\n",
        "\n",
        "\n",
        "#07.Write a Python program to:\n",
        "\n",
        "#● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Load the California Housing dataset\n",
        "housing = fetch_california_housing(as_frame=True)\n",
        "X = housing.data  # Features\n",
        "y = housing.target  # Target variable (median house value)\n",
        "\n",
        "# 2. Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Initialize the GradientBoostingRegressor model\n",
        "# You can adjust hyperparameters like n_estimators, learning_rate, max_depth, etc.\n",
        "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# 4. Train the model on the training data\n",
        "gbr.fit(X_train, y_train)\n",
        "\n",
        "# 5. Make predictions on the test data\n",
        "y_pred = gbr.predict(X_test)\n",
        "\n",
        "# 6. Evaluate the model performance\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfbHVUTX4X4H",
        "outputId": "1e0e7733-b7f8-4fbc-93d6-673425741f38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#07-\n",
        "\n",
        "#Datasets:\n",
        "#● Use sklearn.datasets.load_breast_cancer() for classification tasks.\n",
        "#● Use sklearn.datasets.fetch_california_housing() for regression tasks\n",
        "\n",
        "\n",
        "#7: Write a Python program to:\n",
        "\n",
        "#● Evaluate performance using R-squared score\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# 1. Load the California Housing dataset\n",
        "housing = fetch_california_housing(as_frame=True)\n",
        "X = housing.data  # Features\n",
        "y = housing.target  # Target variable (Median House Value)\n",
        "\n",
        "# 2. Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Initialize and train the Gradient Boosting Regressor\n",
        "# You can adjust hyperparameters like n_estimators, learning_rate, max_depth for better performance\n",
        "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gbr.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions on the test set\n",
        "y_pred = gbr.predict(X_test)\n",
        "\n",
        "# 5. Evaluate performance using R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"R-squared score: {r2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GfzUrYh5kip",
        "outputId": "c6f43530-86df-4640-85e4-4204217146f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared score: 0.7756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#08: Write a Python program to:\n",
        "\n",
        "#● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "\n",
        "#● Tune the learning rate using GridSearchCV\n",
        "\n",
        "#● Print the best parameters and accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the XGBoost Classifier\n",
        "xgb_model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV, focusing on 'learning_rate'\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "\n",
        "# Perform the grid search to tune the learning rate\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found by GridSearchCV\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# Get the best estimator (model with the best parameters)\n",
        "best_xgb_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred = best_xgb_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy on the test set: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90HCF-gSCQu9",
        "outputId": "4f3bb1f9-0ab7-48b8-f0ac-0c72efb3558d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "Best parameters: {'learning_rate': 0.2}\n",
            "Accuracy on the test set: 0.9561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [16:49:37] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    #9: Write a Python program to:\n",
        "\n",
        "     #● Train a CatBoost Classifier\n",
        "\n",
        "\n",
        "    from catboost import CatBoostClassifier, Pool\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.datasets import load_iris # Example dataset\n",
        "\n",
        "    # Load a dataset (e.g., Iris)\n",
        "    data = load_iris()\n",
        "    X = data.data\n",
        "    y = data.target\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    # Initialize CatBoostClassifier\n",
        "    model = CatBoostClassifier(\n",
        "        iterations=100,         # Number of boosting iterations\n",
        "        learning_rate=0.1,      # Step size shrinkage to prevent overfitting\n",
        "        depth=6,                # Depth of the trees\n",
        "        loss_function='MultiClass', # Loss function for multi-class classification\n",
        "        verbose=10              # Print training progress every 10 iterations\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
        "    from sklearn.metrics import accuracy_score\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Example with categorical features (assuming column 0 and 1 are categorical)\n",
        "# cat_features = [0, 1]\n",
        "# model.fit(X_train, y_train, cat_features=cat_features, eval_set=(X_test, y_test))\n",
        "\n",
        "#(There a problem in pip install catboost so there is showing an error in the output,kiundly consider that )\n"
      ],
      "metadata": {
        "id": "qO79GTGCYEga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9: Write a Python program to:\n",
        "\n",
        "\n",
        "#● Plot the confusion matrix using seaborn\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Example data\n",
        "y_true = np.array(['Dog', 'Not Dog', 'Dog', 'Not Dog', 'Dog', 'Dog', 'Not Dog'])\n",
        "y_pred = np.array(['Dog', 'Dog', 'Dog', 'Not Dog', 'Not Dog', 'Dog', 'Not Dog'])\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=['Dog', 'Not Dog'])\n",
        "\n",
        "import pandas as pd\n",
        "cm_df = pd.DataFrame(cm, index=['Actual Dog', 'Actual Not Dog'], columns=['Predicted Dog', 'Predicted Not Dog'])\n",
        "\n",
        "plt.figure(figsize=(8, 6)) # Adjust figure size as needed\n",
        "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues') # annot=True to show values, fmt='d' for integer format\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "tjIyiogR_19n",
        "outputId": "dfed4296-5606-4083-98df-8a1f53fc7bd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o0ouMZzZDA0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.You're working for a FinTech company trying to predict loan default using customer demographics and transaction behavior.\n",
        "\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and categorical features.\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "\n",
        "● Data preprocessing & handling missing/categorical  values\n",
        "\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "\n",
        "● Hyperparameter tuning strategy\n",
        "\n",
        "● Evaluation metrics you'd choose and why\n",
        "\n",
        "● How the business would benefit from your model\n",
        "\n",
        "\n",
        "**ANSWER-**\n",
        "\n",
        "\n",
        "\n",
        "1.Data Preprocessing & Handling Missing/Categorical Values\n",
        "\n",
        "Missing Values:\n",
        "\n",
        "For numerical features: Impute missing values using techniques like mean, median, or more sophisticated methods like K-Nearest Neighbors (KNN) imputation.\n",
        "For categorical features: Impute missing values with the mode or a separate \"missing\" category.\n",
        "\n",
        "Categorical Values:\n",
        "\n",
        "Encode categorical features using methods like One-Hot Encoding for nominal variables or Label Encoding/Ordinal Encoding for ordinal variables, depending on the nature of the categories.\n",
        "\n",
        "Imbalanced Dataset:\n",
        "\n",
        "Address class imbalance using techniques such as:\n",
        "\n",
        "Oversampling:\n",
        "\n",
        "Synthetic Minority Over-sampling Technique (SMOTE) to create synthetic samples of the minority class.\n",
        "\n",
        "Undersampling:\n",
        "\n",
        "Random undersampling of the majority class (use with caution as it can lead to information loss).\n",
        "\n",
        "Cost-sensitive learning:\n",
        "\n",
        "Assigning different weights to misclassifications of minority and majority classes during model training.\n",
        "\n",
        "2.Choice between AdaBoost, XGBoost, or CatBoost\n",
        "\n",
        "XGBoost\n",
        "\n",
        "is generally a strong choice due to its efficiency, scalability, and robust handling of various data types and missing values. It's often a good starting point for its performance and flexibility.\n",
        "\n",
        "CatBoost\n",
        "\n",
        "excels in handling categorical features directly without explicit one-hot encoding, which can be advantageous for datasets with many high-cardinality categorical variables.\n",
        "\n",
        "AdaBoost\n",
        "\n",
        "is simpler and can be effective, but often less performant than gradient boosting methods like XGBoost or CatBoost on complex datasets.\n",
        "Recommendation: For this scenario, XGBoost is a robust and widely used algorithm that can effectively handle the described dataset characteristics. If categorical features are a dominant and complex aspect, CatBoost could be a strong alternative.\n",
        "\n",
        "3.Hyperparameter Tuning Strategy\n",
        "\n",
        "Grid Search\n",
        "\n",
        "or Random Search with cross-validation can be used to explore a predefined range of hyperparameter values.\n",
        "\n",
        "Bayesian Optimization\n",
        "\n",
        "offers a more efficient approach to hyperparameter tuning by building a probabilistic model of the objective function.\n",
        "\n",
        "Early stopping\n",
        "\n",
        "can be employed during training to prevent overfitting and optimize the number of boosting rounds.\n",
        "\n",
        "4.Evaluation Metrics\n",
        "\n",
        "Precision, Recall, and F1-score:\n",
        "\n",
        "Crucial for imbalanced datasets as they provide insights into the model's performance on both minority (default) and majority (non-default) classes, unlike accuracy which can be misleading.\n",
        "\n",
        "Area Under the Receiver Operating Characteristic Curve (AUC-ROC):\n",
        "\n",
        "Measures the model's ability to distinguish between positive and negative classes across various thresholds.\n",
        "\n",
        "Confusion Matrix:\n",
        "\n",
        "Provides a detailed breakdown of true positives, true negatives, false positives, and false negatives.\n",
        "\n",
        "Why these metrics? For loan default prediction, misclassifying a defaulting customer as non-defaulting (false negative) can be very costly for the FinTech company. Precision, Recall, F1-score, and AUC-ROC are more informative than simple accuracy in assessing the model's ability to correctly identify potential defaulters, especially given the imbalanced nature of the dataset.\n",
        "\n",
        "5.Business Benefits of the Model\n",
        "\n",
        "Reduced Financial Losses:\n",
        "\n",
        "By accurately predicting loan defaults, the company can mitigate financial risks and reduce losses associated with non-performing loans.\n",
        "\n",
        "Improved Loan Portfolio Quality:\n",
        "\n",
        "The model enables better risk assessment during loan origination, leading to a healthier and more profitable loan portfolio.\n",
        "\n",
        "Optimized Resource Allocation:\n",
        "\n",
        "Resources can be allocated more effectively by focusing collection efforts on high-risk individuals and offering tailored products to low-risk customers.\n",
        "\n",
        "Enhanced Customer Experience (for non-defaulters):\n",
        "\n",
        "By identifying high-risk individuals, the company can offer more appropriate loan terms or alternative financial products, potentially preventing future defaults and improving customer relationships.\n",
        "\n",
        "Data-Driven Decision Making:\n",
        "\n",
        "The model provides actionable insights into factors contributing to loan default, enabling the company to refine its lending policies and strategies.\n",
        "\n"
      ],
      "metadata": {
        "id": "Bbx9KmCpHcc6"
      }
    }
  ]
}